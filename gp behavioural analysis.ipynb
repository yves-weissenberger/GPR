{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from skimage import data, color\n",
    "import scipy.ndimage as ndimage\n",
    "import h5py\n",
    "import scipy.io as spio\n",
    "import scipy as sp\n",
    "\n",
    "import sys\n",
    "import seaborn\n",
    "seaborn.set(font_scale=2)\n",
    "seaborn.set_style('whitegrid')\n",
    "clrs = seaborn.color_palette()\n",
    "from multiprocessing.dummy import Pool \n",
    "\n",
    "sys.path.append('/home/yves/Documents/')\n",
    "import twoptb as MP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Explanation for What is going on\n",
    "\n",
    "\n",
    "Inference in the gaussian process model works as follows. You assume a latent function f(x) and place a gaussian \n",
    "process prior over this. This means that you assume\n",
    "\n",
    "$\\pi(x) = p(y=1 | x) = \\Phi(f(x))$\n",
    "\n",
    "where $\\Phi$ is a cumulative gaussian (or logistic)\n",
    "\n",
    "Inference of a novel tuple $(x_*,y_*)$ then involes a two step procedure of first computing the distribution of the latent variable $p(f_* | X,y,x_*)$ and then using this for prediction\n",
    "\n",
    "We can rewrite this as (where $D = (X,y,x_*)$ )\n",
    "\n",
    "$p(f_* | D) =  \\displaystyle\\int p(f_*,f | D) df =  \\displaystyle\\int p(f|D) p(f* | D, f) \\ df =  \\displaystyle\\int p(f | X,y,x_*) p(f_* | f, X, x_*, y) df$\n",
    "\n",
    "then since $f_*$ is conditionionally independent of y given f and f is independent of $x_*$\n",
    "\n",
    "$p(f_* | X,y,x_*) =  \\displaystyle\\int p(f | X,y) p(f_* | f, X, x_*) df $\n",
    "\n",
    "\n",
    "In this, the term $ p(f | X,y) = \\displaystyle\\frac{p(y|f)p(f|X)}{p(y|X)}$ is the posterior over latent variables\n",
    "\n",
    "## Laplace Approximation\n",
    "\n",
    "The laplace approximation makes a gaussian approximation to the posterior, assuming that \n",
    "\n",
    "$p(f | X, y) \\approx q(f | X,y)$ where $q \\sim \\displaystyle\\mathcal{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
